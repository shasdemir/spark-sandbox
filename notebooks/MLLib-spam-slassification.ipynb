{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "val sc = spark\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.feature.HashingTF\n",
    "import org.apache.spark.mllib.classification.LogisticRegressionWithSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "/Users/sukruhasdemir/Repos/Courses/spark-sandbox/data/spam-classification/ham.txt MappedRDD[3] at textFile at <console>:19"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataLocation = \"/Users/sukruhasdemir/Repos/Courses/spark-sandbox/data/spam-classification/\"\n",
    "val spamText = sc.textFile(dataLocation + \"spam.txt\")\n",
    "val normalText = sc.textFile(dataLocation + \"ham.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.mllib.feature.HashingTF@318c6f56"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "val tf = new HashingTF(numFeatures = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "Array(Dear sir, I am a Prince in a far kingdom you have not heard of.  I want to send you money via wire transfer so please ..., Get Viagra real cheap!  Send money right away to ..., Oh my gosh you can be really strong too with these drugs found in the rainforest. Get them cheap right now ..., YOUR COMPUTER HAS BEEN INFECTED!  YOU MUST RESET YOUR PASSWORD.  Reply to this email with your password and SSN ..., THIS IS NOT A SCAM!  Send money and get access to awesome stuff really cheap and never have to ...)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spamText.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": "Task not serializable",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Task not serializable",
      "    org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:166)",
      "    org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:158)",
      "    org.apache.spark.SparkContext.clean(SparkContext.scala:1478)",
      "    org.apache.spark.rdd.RDD.map(RDD.scala:288)"
     ]
    }
   ],
   "source": [
    "spamText.map(thing => tf.transform(thing)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": "Task not serializable",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Task not serializable",
      "    org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:166)",
      "    org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:158)",
      "    org.apache.spark.SparkContext.clean(SparkContext.scala:1478)",
      "    org.apache.spark.rdd.RDD.map(RDD.scala:288)"
     ]
    }
   ],
   "source": [
    "val spamFeatures = spamText.map(email => tf.transform(email.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val normalFeatures = normalText.map(email => tf.transform(email.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val positiveExamples = spamFeatues.map(LabeledPoint(1, _))\n",
    "val negativeExamples = normalFeatues.map(LabeledPoint(0, _))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val trainingData = positiveExamples union negativeExamples\n",
    "trainingData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val model = new LogisticRegressionWithSGD().run(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "(10000,[73,2149,2337,4801,4849,5678,5693,9401,9776],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val posTest = tf.transform(\"O M G GET cheap stuff by sending money to ...\".split(\" \"))\n",
    "val negTest = tf.transform(\"Hi Dad, I started studying Spark the other ...\".split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "println(model.predict(posTest))\n",
    "println(model.predict(negTest))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 1.2 (Scala 2.10)",
   "language": "scala",
   "name": "spark-1.2-scala-2.10"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": "scala",
   "mimetype": "text/x-scala",
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
